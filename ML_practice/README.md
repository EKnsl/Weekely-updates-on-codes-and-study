# Week1

Date : 19/05/2022

https://ml.howtocode.dev/

[ML basics(Lec 1.1 to 2.3 of the playlist)](https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN)

---------------------------------------------------------------------------------------------------------
Date : 20/05/2022

[Linear Regression](https://github.com/mhuzaifadev/mlzero_to_hero/tree/main/04_Simple%20_Linear_Regression)

-----------------------------------------------------------------------------------------------------------
Date : 23/05/2022

[lec 2.4 to 3.4 (Linear Regression with one variable and Linear Algebra upto matrix matrix multiplication)](https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN)

Additional : 

[ML intro](https://ml.howtocode.dev/)

[why matrix multiplication is not commutative i.e order matters](https://www.quora.com/Why-is-the-multiplication-of-matrices-not-a-commutative-property-so-that-AB-neq-BA)

----------------------------------------------------------------------------------------------------------

Date : 24/05/2022

[Linear Algebra review](https://towardsdatascience.com/linear-algebra-for-machine-learning-22f1d8aea83c)

[Lec 3.4 to 3.6(Linear Algebra) and Lecture 4.1 — Linear Regression With Multiple Variables-(Multiple Features)](https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN)

[Multiple Linear Regreassion notebook with startup data](https://github.com/mhuzaifadev/mlzero_to_hero/tree/main/05%20Multiple%20Linear%20Regression)

additional resources : 

Example of using categorical features : [Multiple Linear Regression: A quick Introduction](https://www.askpython.com/python/examples/multiple-linear-regression)

------------------------------------------------------------------------------------------------------------------------------------------

Date : 25/05/2022

Linear Regression with Multiple Variable: [Lecture 4.1 - 4.7 of this playlist.](https://www.youtube.com/watch?v=PPLop4L2eGk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN)

additional resources :
[Why One-Hot Encode Data in Machine Learning?](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning)

[Use ColumnTransformer to apply different preprocessing to different columns](https://www.youtube.com/watch?v=NGq8wnH5VSo)

[How column Transformers work](https://www.analyticsvidhya.com/blog/2021/05/understanding-column-transformer-and-machine-learning-pipelines/)

------------------------------------------------------------------------------------------------------------------------------------------------------

Date : 26/05/2022

* ## Practise Logistic Regression

    [My Logistic Regression Notebook](https://github.com/EKnsl/Weekely-updates-on-codes-and-study/blob/main/ML_practice/week1_ML_%20Intro_and_supervised_learing/code/Logistic_Regression.ipynb)

    * Prediction of survival with titanic dataset using Logistic regression 

    * Performance evaluation using accuracy precison, recall and f1 score

    related resources : 

    https://datascienceplus.com/logistic-regression-with-python-using-titanic-data/

    [kaggle notenook](https://www.kaggle.com/code/mnassrib/titanic-logistic-regression-with-python/notebook)

    [Feature Selection Techniques for regression](https://machinelearningmastery.com/feature-selection-for-regression-data/)

    -------------------------------------------------------------------------------------------------------------------------------------

Date : 30/05/2022

* ## Logistic Regression Basic concepts

    * Learnt about

        * Logistic function  
        
        * non-linear decision boundaries

        * A better cost function than sqaured error for classification with non linear decision boundary
         
        * Some optimization algorithms that often converge faster than Gradiant descent (according to Andrew N.G):
            * [Conjugate Gradient Descent](https://ikuz.eu/machine-learning-and-computer-science/the-concept-of-conjugate-gradient-descent-in-python/)
            
            * [BFGS](https://machinelearningmastery.com/bfgs-optimization-in-python/)
            
            * L-BFGS : 
            [How L-BFGS works](https://stats.stackexchange.com/questions/284712/how-does-the-l-bfgs-work),
            [L-BFGS method basics with python code](https://www.earthinversion.com/techniques/the-L-BFGS-optimization-method/)

        * One vs rest or One vs all for multiclass classification (choose 1 class from k classes)
            
            [One-vs-Rest and One-vs-One for Multi-Class Classification](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/ )
            
            [Logistic regression  Multiclass - One-vs-rest classification](https://www.youtube.com/watch?v=EYXSve6T5BU)
            
            [Logistic Regression Mutliclass Classification(OneVsRest)](https://www.youtube.com/watch?v=V8fS0T_ktn4)

    * Completed [Lecture 6.1 - 6.7 of this playlist.](https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN)

------------------------------------------------------------------------------------------------------------------------------------

# Week2


Date : 02/06/2022

* ## Study conditional probability , Naive Bayes and K Nearest Neighbour
    
    [Naive Bayes practice](https://github.com/EKnsl/Weekely-updates-on-codes-and-study/tree/main/ML_practice/week2_Supervised_Learning/Naive%20Bayes)

    [K Nearest Neighbour practice](https://github.com/EKnsl/Weekely-updates-on-codes-and-study/tree/main/ML_practice/week2_Supervised_Learning/K%20Nearest%20Neighbour)


Date : 13/06/2022

* ## Study and practice support vector machine (SVM)
    
    * completed lecture 12.1 to 12.6 of  [this playlist](https://www.youtube.com/watch?v=FCUBwP-JTsA&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=75) 

    * [SVM practice notebook](https://github.com/EKnsl/Weekely-updates-on-codes-and-study/blob/main/ML_practice/week2_Supervised_Learning/Support%20Vector%20Machine/Support_Vector_Machines.ipynb)

---------------------------------------------------------------------------------------------------------------------------------------------------

# Week 03 | Part 1: Tuning and Performance Metrics of ML Model

Date : 13/06/20222

* ## Learnt basics of regularization and bias variance tradeoff

    * Completed Regularization in Traditional ML | Part 1: lecture [7.1 to 7.4 of this playlist](https://www.youtube.com/watch?v=u73PU6Qwl1I&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=39)

    * Completed Regularization in Traditional ML | Part 2: lecture [10.1 to 10.7 of this playlist](https://www.youtube.com/watch?v=sZSKGNbrwus&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=58)

Date : 21/06/20222

* Completed Performance Metrics: [Lecture 11.1 - 11.5 of this playlist.](https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN)


Date : 22/06/20222


* study [Regularization for Simplicity](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/video-lecture) 

* read [Regularization for Simplicity: L₂ Regularization](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization), [Lambda](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda), [Generalization: Peril of Overfitting](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting#ockham)

Date : 23/06/20222

* study [Regularization in Traditional ML](http://ethen8181.github.io/machine-learning/regularization/regularization.html) 

* [regularization pratice notebook](https://github.com/EKnsl/Weekely-updates-on-codes-and-study/blob/main/ML_practice/week3_part1/Regularization/regularization.ipynb)

# Week 03 | Part 2: Ensemble Learning

Date : 26/06/20222

* studied Decison Tree : [Decision Tree Explained](https://www.youtube.com/watch?v=7VeUPuFGJHk) ,
          [Decision Tree Summary.](https://www.youtube.com/watch?v=tNa99PG8hR8)  
          
* studied Random Forest : [part1](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ) ,
          [part2](https://www.youtube.com/watch?v=nyxTdL_4Q-Q)









    
    